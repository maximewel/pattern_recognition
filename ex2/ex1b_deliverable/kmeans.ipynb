{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "data classes, general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from enum import Enum, auto\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_TRAIN = \"./mnist_small/train.csv\"\n",
    "KNN_TEST = \"./mnist_small/test.csv\"\n",
    "\n",
    "names = [\"label\"] + [f\"Pix {i}\" for i in range(28*28)]\n",
    "\n",
    "train_df = pd.read_csv(KNN_TRAIN, names=names)\n",
    "test_df = pd.read_csv(KNN_TEST, names=names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding part\n",
    "The following sections contain the coding part of the project\n",
    "\n",
    "* Image visualization (helper tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image(array: np.ndarray):\n",
    "    \"\"\"Visualize the given flattened image. Assume 28*28 len\"\"\"\n",
    "    image = array.reshape(28, 28)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(f\"Cluster center\") \n",
    "    plt.imshow(image, cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "    plt.show(True)\n",
    "\n",
    "def add_cluster_center_as_ax(cluster_center: np.ndarray, cluster_index: int, ax):\n",
    "    image = cluster_center.reshape(28, 28)\n",
    "\n",
    "    ax.set_title(f\"Clust.{cluster_index}\")\n",
    "    ax.imshow(image, cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* trivial score computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerminateCondition(Enum):\n",
    "    CHANGED_CLUSTER = 0b01\n",
    "    CHANGED_CENTER = 0b10\n",
    "\n",
    "@dataclass\n",
    "class Score():\n",
    "    \"\"\"Hold the score for a training set evalutation. Contain the score evalutation method\"\"\"\n",
    "    total: int\n",
    "    success: int\n",
    "    failures: int\n",
    "\n",
    "    accuracy: float\n",
    "\n",
    "    @classmethod\n",
    "    def compute_score(total: int, successes: int, failures: int) -> Score:\n",
    "        \"\"\"Compute the score from the given successes / failures\n",
    "        \n",
    "        Returns\n",
    "        -----\n",
    "            A score object containing the information\n",
    "        \"\"\"\n",
    "        accuracy = successes / total\n",
    "\n",
    "        return Score(total, successes, failures, accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Custom classifier, heavily coupled to test dataset but optimized for efficient tests on k via memoization of the distance map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KmeansClassifier:\n",
    "    \"\"\"Class encapsulating the logic of the KNN algorithm\"\"\"\n",
    "    #Work with arrays to be more efficient\n",
    "    data: np.ndarray\n",
    "    labels: np.ndarray\n",
    "\n",
    "    cluster_map: np.ndarray\n",
    "    k: int\n",
    "\n",
    "    def __init__(self, train_df: pd.DataFrame) -> None:\n",
    "        \"\"\"Init the KNN classifier with the training and testing dataset\"\"\"\n",
    "        self.labels = train_df.loc[:, 'label'].to_numpy()\n",
    "        self.data = train_df.drop(['label'], axis=1, inplace=False).to_numpy()\n",
    "        print(f\"K-means loaded with {len(self.data)} data\")\n",
    "    \n",
    "    def cluster_data(self, cluster_index: int) -> np.ndarray:\n",
    "        \"\"\"Return the datapoint belonging to cluster index\"\"\"\n",
    "        return self.data[self.cluster_map == cluster_index]\n",
    "\n",
    "    def single_linkage_distance(self, cluster_ind_from: int, cluster_ind_to: int):\n",
    "        \"\"\"Determing the min distance between pts of clst1 to points of clst2\"\"\"\n",
    "        cluster_from = self.cluster_data(cluster_ind_from)\n",
    "        cluster_to = self.cluster_data(cluster_ind_to)\n",
    "\n",
    "        min_point_distance = []\n",
    "        for point in cluster_from:\n",
    "            distances = self.euclidian_distances(point, cluster_to)\n",
    "            min_point_distance.append(np.min(distances))\n",
    "        \n",
    "        return np.min(min_point_distance)\n",
    "\n",
    "    def cluster_diameter(self, cluster_index: int) -> int:\n",
    "        \"\"\"Return the diameter of said cluster\"\"\"\n",
    "        cluster_data = self.cluster_data(cluster_index)\n",
    "\n",
    "        max_distances = []\n",
    "        for point in cluster_data:\n",
    "            distances = self.euclidian_distances(point, cluster_data)\n",
    "            max_distances.append(np.max(distances))\n",
    "        \n",
    "        return np.max(max_distances)\n",
    "\n",
    "    def dunn_index(self):\n",
    "        cluster_min_distances = []\n",
    "        cluster_diameters = []\n",
    "        for cluster_index in range(self.k):\n",
    "\n",
    "            #First: Calculate minimal single linkage distance between clusters\n",
    "            cluster_distances = []\n",
    "            for cluster_index_to in range(self.k):\n",
    "                if cluster_index_to == cluster_index:\n",
    "                    continue \n",
    "                single_linkage_distance = self.single_linkage_distance(cluster_index, cluster_index_to)\n",
    "                cluster_distances.append(single_linkage_distance)\n",
    "            #Add this cluster's minimum linkage distance to all min distances\n",
    "            cluster_min_distances.append(np.min(cluster_distances))\n",
    "\n",
    "\n",
    "            #Second: Calculate diameter for each cluster\n",
    "            diameter = self.cluster_diameter(cluster_index)\n",
    "            cluster_diameters.append(diameter)\n",
    "\n",
    "        dunn_index = np.min(cluster_min_distances) / np.max(cluster_diameters)\n",
    "        return dunn_index\n",
    "\n",
    "    def davis_bouldin_index(self):\n",
    "        \"\"\"Return the davis bouldin index\"\"\"\n",
    "        #Create repository of Di, Mi per cluster\n",
    "        d_m_clusters = []\n",
    "        for cluster_index in range(self.k):\n",
    "            cluster_data = self.cluster_data(cluster_index)\n",
    "\n",
    "            m = np.mean(cluster_data, axis=0)\n",
    "            d = np.sum(self.euclidian_distances(m, cluster_data)) / len(cluster_data)\n",
    "            d_m_clusters.append((d, m))\n",
    "        \n",
    "        #Now calculate each Rij easily as we have the repositories of centers and distances\n",
    "        Ri_list = []\n",
    "        for i, (di,mi) in enumerate(d_m_clusters):\n",
    "            Rij_list = []\n",
    "            for j, (dj, mj) in enumerate(d_m_clusters):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                # Put into one dim array, and get first value as distance function expects to compare array to matrix\n",
    "                center_distances = self.euclidian_distances([mi], [mj])[0] \n",
    "                Rij = (di + dj) / center_distances\n",
    "                Rij_list.append(Rij)\n",
    "            Ri_list.append(np.max(Rij_list))\n",
    "        \n",
    "        davis_bouldin_index = sum(Ri_list) / self.k\n",
    "        return davis_bouldin_index\n",
    "\n",
    "    def euclidian_distances(self, point: np.ndarray, matrix: np.ndarray):\n",
    "        \"\"\"Compute the distance as the difference in intensity level pixel-wise between the input datapoint and the given matrix.\n",
    "        \n",
    "        Returns\n",
    "        ----\n",
    "            A matrix of distance between the point and each matrix's data point.\n",
    "            The matrix thus has a size of (N, 1): I column as the distance is a singular value,\n",
    "            N rows as each row is the distance to a matrix's datapoint\n",
    "        \"\"\"\n",
    "        #use numpy functions as they are made to be efficient with vecotrized input\n",
    "        return np.sqrt(np.sum(np.power(np.subtract(point, matrix), 2), axis=1))\n",
    "\n",
    "    def is_terminated(self, i: int, i_max: int, has_changed_cluster: bool, has_changed_center: bool, \n",
    "        terminate_flags: List[TerminateCondition]) -> bool:\n",
    "        if i >= i_max:\n",
    "            print(f\"\\nMax iterations reached: {i}\")\n",
    "            return True\n",
    "        if TerminateCondition.CHANGED_CLUSTER in terminate_flags and not has_changed_cluster:\n",
    "            print(f\"\\nNo cluster changes detected\")\n",
    "            return True\n",
    "        if TerminateCondition.CHANGED_CENTER in terminate_flags and not has_changed_center:\n",
    "            print(f\"\\nNo center changes detected\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def clusterize(self, k: int, iterations: int, terminate_flags: List[TerminateCondition]) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Classify each item on the test dataset and yields an evaluation score\n",
    "        \n",
    "        Args\n",
    "        -----\n",
    "            k: int - The number of expected clusters\n",
    "            iterations: int - \n",
    "            terminate_condition: int - Additional terminate condition, see Terminate condition enums\n",
    "\n",
    "        Returns\n",
    "        ----\n",
    "            A score object\n",
    "        \"\"\"\n",
    "        #Chose k random centers, init clusters with single point. Avoid same-point centers.\n",
    "        self.k = k\n",
    "        rand_indexes = []\n",
    "        for _ in range(k):\n",
    "            randToss = None\n",
    "            while randToss is None or randToss in rand_indexes:\n",
    "                randToss = random.randrange(0, len(self.data))\n",
    "            rand_indexes.append(randToss)\n",
    "\n",
    "        centers = np.take(self.data, rand_indexes, axis=0)\n",
    "            \n",
    "        #Init \"cluster identification\" array for each datapoint\n",
    "        self.cluster_map = np.zeros((len(self.data)), dtype=int)\n",
    "\n",
    "        has_changed_cluster = True\n",
    "        has_changed_center = True\n",
    "\n",
    "        i = 0\n",
    "        while not self.is_terminated(i, iterations, has_changed_cluster, has_changed_center, terminate_flags):\n",
    "            i += 1\n",
    "            print(f\"\\rIteration <{i:5}>\", end=\"\")\n",
    "            has_changed_cluster = False\n",
    "            has_changed_center = False\n",
    "\n",
    "            #Reassign each point to centers\n",
    "            for point_index, point in enumerate(self.data):\n",
    "                #Calculate distance map\n",
    "                point_centers_dist_matrix = self.euclidian_distances(point, centers)\n",
    "\n",
    "                #Find index of nearest center, that's the new cluster index\n",
    "                new_cluster_index = np.argmin(point_centers_dist_matrix)\n",
    "\n",
    "                #Verify if it has changed\n",
    "                current_cluster_index = self.cluster_map[point_index]\n",
    "                if current_cluster_index != new_cluster_index:\n",
    "                    has_changed_cluster = True\n",
    "                    self.cluster_map[point_index] = new_cluster_index\n",
    "\n",
    "            #Re-calculate centers\n",
    "            for cluster_index in range(k):\n",
    "                #Get all data assigned to this cluster using ndarray binary mask on cluster index\n",
    "                cluster_points = self.data[self.cluster_map == cluster_index]\n",
    "                #cluster_points = np.take(self.data, (self.cluster_map == cluster_index), axis=0)\n",
    "\n",
    "                #Center is simply the new mean of this data\n",
    "                new_cluster_mean = np.mean(cluster_points, axis=0)\n",
    "\n",
    "                #Track wheter the center has changed\n",
    "                current_cluster_mean = centers[cluster_index]\n",
    "                if not np.array_equal(current_cluster_mean, new_cluster_mean):\n",
    "                    has_changed_center = True\n",
    "                    centers[cluster_index] = new_cluster_mean\n",
    "\n",
    "        return self.cluster_map, centers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main functions\n",
    "Function used to perform the assignement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterize_with_k(k: int) -> tuple[np.ndarray, np.ndarray, KmeansClassifier]:\n",
    "    \"\"\"Clusterize with the given K clusters.\n",
    "    Uses 100 iterations as this is generally enough for the k-means to stop beforehand because of no changes detected.\n",
    "    \n",
    "    Returns\n",
    "    -----\n",
    "        The tzple (cluster_map, cluster_centers)\n",
    "    \"\"\"\n",
    "    kmeans_classifier = KmeansClassifier(train_df)\n",
    "    max_iter = 100\n",
    "\n",
    "    cluster_map, cluster_centers = kmeans_classifier.clusterize(k, max_iter, (TerminateCondition.CHANGED_CENTER, TerminateCondition.CHANGED_CLUSTER))\n",
    "\n",
    "    return cluster_map, cluster_centers, kmeans_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster analysis\n",
    "Make an analysis of the data repartition over the final clusterized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse(classifier: KmeansClassifier, labels: np.ndarray, cluster_map: np.ndarray):\n",
    "    #Calculate each cluster's validity, using different methods\n",
    "\n",
    "    #Anylse repartition with classes\n",
    "    print(f\"Analysing clusters...\")\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            #get all cluster values corresponding to the class\n",
    "            class_value = 5*i + j\n",
    "            binary_mask = (labels == class_value)\n",
    "            cluster_values = cluster_map[binary_mask]\n",
    "            \n",
    "            #Get most used cluster\n",
    "            sorted_freq_labels = Counter(cluster_values).most_common()\n",
    "            most_freq_cluster = int(sorted_freq_labels[0][0])\n",
    "\n",
    "            print(f\"Label {class_value} has {np.count_nonzero(binary_mask)} elements. They are spread around {len(np.unique(cluster_values))} clusters\")\n",
    "            print(f\"Cluster spread: {sorted_freq_labels}\")\n",
    "            counts = [count for val, count in sorted_freq_labels]\n",
    "            together_ratio = 100 * (counts[0] / sum(counts))\n",
    "            print(f\"Class {class_value} is mostly represented by cluster {most_freq_cluster} : {counts[0]}/{sum(counts)} ({together_ratio:.2f})%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data repartition\n",
    "Plot used to see the overall data repartition between labels and clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_repartition(labels, cluster_map):\n",
    "    fig, axs = plt.subplots(2)\n",
    "    fig.suptitle('Vertically stacked subplots')\n",
    "\n",
    "    axs[0].bar(np.linspace(0, len(train_df), len(train_df)), labels)\n",
    "    axs[0].set_title(\"Index - Labels\")\n",
    "    axs[0].set_xlabel(\"Index\")\n",
    "    axs[0].set_ylabel(\"Label\")\n",
    "\n",
    "    axs[1].bar(np.linspace(0, len(train_df), len(train_df)), cluster_map)\n",
    "    axs[1].set_title(\"Index - clusterized\")\n",
    "    axs[1].set_xlabel(\"Index\")\n",
    "    axs[1].set_ylabel(\"Cluster\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Piechart\n",
    "Used to see the volume repratition for labels and clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_piechart(labels, cluster_map):\n",
    "    fig, axes = plt.subplots(2)\n",
    "    fig.suptitle('Data repratition over initial dataset and resulting clusterized dataset')\n",
    "\n",
    "    #Draw labels pie chart\n",
    "    slices = []\n",
    "    activities = []\n",
    "    for i in range(10):\n",
    "        slices.append(np.count_nonzero( (labels == i) )) \n",
    "        activities.append(f\"nbr_{i}\")\n",
    "\n",
    "    axes[0].pie(slices,\n",
    "        labels = activities,\n",
    "        shadow = True)\n",
    "\n",
    "    #Draw clusters pie chart\n",
    "    slices = []\n",
    "    activities = []\n",
    "    for i in range(len(np.unique(cluster_map))):\n",
    "        slices.append(np.count_nonzero( (cluster_map == i) ))\n",
    "        activities.append(f\"cluster_{i}\")\n",
    "\n",
    "    axes[1].pie(slices,\n",
    "        labels = activities,\n",
    "        shadow = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function used to show repartition of labeled data into their clusters\n",
    "This repartition is focused on original data, this would not be possible with trully unsupervised value.\n",
    "This is a way to see where each of the drawn number went into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_per_label(labels, cluster_map, cluster_centers):\n",
    "    fig, axs = plt.subplots(2, 5)\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(5):\n",
    "            #get all cluster values corresponding to the class\n",
    "            class_value = 5*i + j\n",
    "            binary_mask = (labels == class_value)\n",
    "            cluster_values = cluster_map[binary_mask]\n",
    "            \n",
    "            #Get most used cluster\n",
    "            sorted_freq_labels = Counter(cluster_values).most_common()\n",
    "            most_freq_cluster = int(sorted_freq_labels[0][0])\n",
    "\n",
    "            #Display class's main cluster \n",
    "            center = cluster_centers[most_freq_cluster]\n",
    "            add_cluster_center_as_ax(center, most_freq_cluster, axs[i][j])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display all clusters to have an idea of the final repartition\n",
    "It is interesting to plot all the final cluster's centers in order to visualize each \"cluster mean\", or \"cluster target\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clusters_visuals(cluster_centers):\n",
    "    cols = 5\n",
    "    rows = int(np.ceil(len(cluster_centers) / cols))\n",
    "    fig, axs = plt.subplots(rows, cols)\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            cluster_ind = 5*i + j\n",
    "            if cluster_ind < len(cluster_centers):\n",
    "                add_cluster_center_as_ax(cluster_centers[cluster_ind], cluster_ind, axs[i][j] if rows>1 else axs[j])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignnement\n",
    "Test the model accuracy with different cluster validation methods\n",
    "\n",
    "* Validation Values for K = {5, 7, 9, 10, 12, 15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "labels = train_df.loc[:, 'label'].to_numpy()\n",
    "out_path =  os.path.join(os.path.abspath(''), \"out\")\n",
    "if os.path.exists(out_path):\n",
    "    archive_path = os.path.join(os.path.abspath(''), \"archive\")\n",
    "    if os.path.exists(archive_path):\n",
    "        os.remove(archive_path)\n",
    "    os.rename(out_path, archive_path)\n",
    "os.mkdir(out_path)\n",
    "\n",
    "Ks = [5, 7, 9, 10, 12, 15]\n",
    "dunn_indexes = []\n",
    "davis_bouldin_indexes = []\n",
    "\n",
    "for k in Ks:\n",
    "    print(f\"Iteration k={k}\")\n",
    "    full_path = os.path.join(out_path, f\"{k}\")    \n",
    "    os.mkdir(full_path)\n",
    "    \n",
    "    cluster_map, cluster_centers, classifier = clusterize_with_k(k)\n",
    "\n",
    "    print(f\"Calculating cluster validation...\")\n",
    "    dunn_index = classifier.dunn_index()\n",
    "    print(f\"Dunn index (The larger = the better): {dunn_index}\")\n",
    "    dunn_indexes.append(dunn_index)\n",
    "\n",
    "    davis_bouldin_index = classifier.davis_bouldin_index()\n",
    "    print(f\"Davis bouldin index (The lower = the better): {davis_bouldin_index}\")\n",
    "    davis_bouldin_indexes.append(davis_bouldin_index)\n",
    "\n",
    "    build_data_repartition(labels, cluster_map)\n",
    "    plt.savefig(f'./out/{k}/data_repartition.pdf')\n",
    "\n",
    "    build_piechart(labels, cluster_map)\n",
    "    plt.savefig(f'./out/{k}/piecharts.pdf')\n",
    "\n",
    "    build_data_per_label(labels, cluster_map, cluster_centers)\n",
    "    plt.savefig(f'./out/{k}/label_repartition.pdf')\n",
    "\n",
    "    build_clusters_visuals(cluster_centers)\n",
    "    plt.savefig(f'./out/{k}/clusters.pdf')\n",
    "    print()\n",
    "\n",
    "print(f\"Computation finished.\")\n",
    "\n",
    "dunn_ordered = sorted(zip(Ks, dunn_indexes), key = lambda k_dun: k_dun[1], reverse=True)\n",
    "print(f\"Best clusters according to Dunn: {dunn_ordered[0]}, ordered: {dunn_ordered}\")\n",
    "\n",
    "davis_sorted = sorted(zip(Ks, davis_bouldin_indexes), key = lambda k_davis: k_davis[1])\n",
    "print(f\"Best clusters according to davis bouldin: {davis_sorted[0]}, ordered: {davis_sorted}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e835e044c40c867db05629037020e41dc9de711a2d3844bbe40049c79692d69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
